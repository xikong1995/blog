![赵露思](https://img-blog.csdnimg.cn/20200906151727458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

# 前言

**兴趣是最好的老师！** 我们知道纯粹的技术是枯燥无味的，将技术和生活结合起来，这样技术才能得到升华。

本文的终结目标就是微博上的漂亮小姐姐！不知道你是否有深夜右键另存为的经历。一个不要紧，成千上万的图片和视频那真是要人命。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906154610130.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

看完本文，那种又累又 low 的日子将一去不复返。轻轻按下回车键，然后出去晃荡一圈，回来打开目录，你想要的就全都有了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906154445642.gif#pic_center)

**特别注意：**

本文不适合零基础同学阅读，但也没有什么高深莫测的东西。这里简单提及下需要准备的前置知识：
1. 需要一定的 html 基础
2. 了解 nodejs，至少跑通过 "hello world" 示例
3. 需要一定的 es6 基础
4. 知道 npm 包的概念

# 一、分析网页微博

## 1. 分析爬虫方向

我们主要爬取微博网页版内容，网址为 [https://weibo.com/](https://weibo.com/)。

通过搜索我们先找到一个小姐姐的主页。博主这里翻了吴宣仪小姐姐的牌子，主页地址为 [https://weibo.com/xuanyi0808](https://weibo.com/xuanyi0808)。

向下滚动页面时，我们可以发现页面会提示加载更多，观察后可以知道每一页会进行两次加载更多，最后显示页码，这时你可以选择下一页或其他页。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906161519693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906161555103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

当我们选择下一页时，会弹出一个登录的弹出框。这说明如果我们想要爬取所有内容需要先登录，对应的就是请求里面携带 cookie 或 token。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090616181387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

## 2. 分析图片地址位置

界面大致看完了，这个时候我们就需要打起精神来，观察其源代码结构。首先我们需要右键然后选择检查，快捷操作就是按 `f12`。

然后我们审查元素，查看图片所在的 html 片段的位置。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906162719683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906162548268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)
一番探索，我们就可以得到图片地址所在的位置，如图所示，img 标签的 src 属性的值就是图片的地址。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906163117504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

## 3. 分析视频地址位置

和图片的操作类似，审查元素，找到视频所对应的代码位置，然后适当调试下，即可找到视频地址所在的位置。这个 `video-source` 的值前面有一个 `fluency=` 需要去掉。

值的注意的是去掉后得到的地址，还需要经过**两次** url 解码，才能真正被浏览器识别。这里给出解码前的地址，有兴趣的朋友可以测试下。

```text
http%253A%252F%252Ff.us.sinaimg.cn%252F002ruDTXlx07vrCFl6IE01041200bUKs0E010.mp4%253Flabel%253Dmp4_720p%2526template%253D1280x720.23.0%2526trans_finger%253Dbead91e89870c048e540ef7cd58c7c03%2526media_id%253D4394384829970833%2526tp%253D8x8A3El%253AYTkl0eM8%2526us%253D0%2526ori%253D1%2526bf%253D4%2526ot%253Dh%2526lp%253D000024lUXC%2526ps%253D4pdsh0%2526uid%253D5yZbZm%2526ab%253D966-g1%2526Expires%253D1599384190%2526ssig%253Dxl8G0Ccp3v%2526KID%253Dunistore%252Cvideo&480=http%3A%2F%2Ff.us.sinaimg.cn%2F0006QOzNlx07vrCEVaN2010412004sx50E010.mp4%3Flabel%3Dmp4_hd%26template%3D852x480.23.0%26trans_finger%3D53d43933e6520536fed61835e8c1d811%26media_id%3D4394384829970833%26tp%3D8x8A3El%3AYTkl0eM8%26us%3D0%26ori%3D1%26bf%3D4%26ot%3Dh%26lp%3D000024lUXC%26ps%3D4pdsh0%26uid%3D5yZbZm%26ab%3D966-g1%26Expires%3D1599384190%26ssig%3DxEwYfeRcPx%26KID%3Dunistore%2Cvideo&720=http%3A%2F%2Ff.us.sinaimg.cn%2F002ruDTXlx07vrCFl6IE01041200bUKs0E010.mp4%3Flabel%3Dmp4_720p%26template%3D1280x720.23.0%26trans_finger%3Dbead91e89870c048e540ef7cd58c7c03%26media_id%3D4394384829970833%26tp%3D8x8A3El%3AYTkl0eM8%26us%3D0%26ori%3D1%26bf%3D4%26ot%3Dh%26lp%3D000024lUXC%26ps%3D4pdsh0%26uid%3D5yZbZm%26ab%3D966-g1%26Expires%3D1599384190%26ssig%3Dxl8G0Ccp3v%26KID%3Dunistore%2Cvideo&1080=&quality_label_list=%5B%7B%22url%22%3A%22http%3A%5C%2F%5C%2Ff.us.sinaimg.cn%5C%2F002ruDTXlx07vrCFl6IE01041200bUKs0E010.mp4%3Flabel%3Dmp4_720p%26template%3D1280x720.23.0%26trans_finger%3Dbead91e89870c048e540ef7cd58c7c03%26media_id%3D4394384829970833%26tp%3D8x8A3El%3AYTkl0eM8%26us%3D0%26ori%3D1%26bf%3D4%26ot%3Dh%26lp%3D000024lUXC%26ps%3D4pdsh0%26uid%3D5yZbZm%26ab%3D966-g1%26Expires%3D1599384190%26ssig%3Dxl8G0Ccp3v%26KID%3Dunistore%2Cvideo%22%2C%22quality_desc%22%3A%22%5Cu9ad8%5Cu6e05%22%2C%22quality_label%22%3A%22720p%22%2C%22quality_icon%22%3A%22%22%2C%22scheme%22%3A%22%22%7D%2C%7B%22url%22%3A%22http%3A%5C%2F%5C%2Ff.us.sinaimg.cn%5C%2F0006QOzNlx07vrCEVaN2010412004sx50E010.mp4%3Flabel%3Dmp4_hd%26template%3D852x480.23.0%26trans_finger%3D53d43933e6520536fed61835e8c1d811%26media_id%3D4394384829970833%26tp%3D8x8A3El%3AYTkl0eM8%26us%3D0%26ori%3D1%26bf%3D4%26ot%3Dh%26lp%3D000024lUXC%26ps%3D4pdsh0%26uid%3D5yZbZm%26ab%3D966-g1%26Expires%3D1599384190%26ssig%3DxEwYfeRcPx%26KID%3Dunistore%2Cvideo%22%2C%22quality_desc%22%3A%22%5Cu6807%5Cu6e05%22%2C%22quality_label%22%3A%22480p%22%2C%22quality_icon%22%3A%22%22%2C%22scheme%22%3A%22%22%7D%2C%7B%22url%22%3A%22http%3A%5C%2F%5C%2Ff.us.sinaimg.cn%5C%2F001xhQxJlx07vrCEzn6w010412002COX0E010.mp4%3Flabel%3Dmp4_ld%26template%3D640x360.23.0%26trans_finger%3D63d241190ef69e700dd33d4bec649dc1%26media_id%3D4394384829970833%26tp%3D8x8A3El%3AYTkl0eM8%26us%3D0%26ori%3D1%26bf%3D4%26ot%3Dh%26lp%3D000024lUXC%26ps%3D4pdsh0%26uid%3D5yZbZm%26ab%3D966-g1%26Expires%3D1599384190%26ssig%3Du8GpBoZ8BG%26KID%3Dunistore%2Cvideo%22%2C%22quality_desc%22%3A%22%5Cu6d41%5Cu7545%22%2C%22quality_label%22%3A%22360p%22%2C%22quality_icon%22%3A%22%22%2C%22scheme%22%3A%22%22%7D%5D&qType=720
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906163855919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

## 4. 触发加载更多

这种加载更多都是通过异步请求去获取的，所以在触发加载更多前，我们先将调试面板切到 Network。经过观察，当我们触发加载更多时，会发送如下请求。

**触发第一次加载更多**

`https://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&domain=100605&is_all=1&pagebar=0&pl_name=Pl_Official_MyProfileFeed__22&id=1006055796662600&script_uri=/xuanyi0808&feed_type=0&page=1&pre_page=1&domain_op=100605&__rnd=1599382219407`

**触发第二次加载更多**

`https://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&domain=100605&is_all=1&pagebar=1&pl_name=Pl_Official_MyProfileFeed__22&id=1006055796662600&script_uri=/xuanyi0808&feed_type=0&page=1&pre_page=1&domain_op=100605&__rnd=1599382484778`

> 这两次的主要区别在 `pagebar` 这个参数，第一次加载时值为 `0`，第二次加载时值为 `1`。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906165854564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

## 5. 进入下一页

这个比较简单，但是别忘了需要先登录。当我们点击下一页时，页面进行了跳转，浏览器地址栏的路径发生了变化。

`https://weibo.com/xuanyi0808?is_search=0&visible=0&is_all=1&is_tag=0&profile_ftype=1&page=2#feedtop`

相信大家一眼就能看出，控制页数主要是 `page` 参数。

## 6. 获取总页数

经过前面的分析，我们目前知道了每一页会有三部分，第一部分是通过访问浏览器地址直接返回的，第二、三部分是通过另外一个接口异步获取的。

现在有一个问题，我们怎么得到总页数呢？前面提到过，当请求第三部分数据时，页面就会出现页面选择器。这个时候，我们就可以得到总页数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200906170809804.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pob3VsZWkxOTk1,size_16,color_FFFFFF,t_70#pic_center)

现在我们已经知道如下信息：
1. 每一页的三部分构成
2. 获取下一页内容
3. 数据的总页数
4. 图片和视频地址的位置

目前食材已全部准备就绪，可以下锅开炒了。

# 二、搭建爬虫项目

## 1. 依赖的 npm 包

在正式开始编程前，博主想先介绍几个 npm 包，这个是我们项目中的核心依赖，大家可以先简单熟悉他们的功能和用法。

**superagent**

官网：[https://superagent.org.cn/](https://superagent.org.cn/)，一个简单的轻量级同构 Ajax 请求库。主要用于 nodejs 服务端的网络请求，相比原生的 nodejs 包，它使用起来更加简单方便。

**cheerio**

官网：[https://cheerio.js.org/](https://cheerio.js.org/)，专为服务器设计的核心 jQuery 的快速，灵活和精益实现。可以帮助我们快速解析 html 文件，获取关键的图片和视频地址。

**async**

官网：[https://caolan.github.io/async/v3/](https://caolan.github.io/async/v3/)，async 是一个实用程序模块，它提供直接，强大的功能来处理异步 JavaScript。主要用于限制同时发出的请求数，如果我们一下子发出大量的图片或视频请求，可能会被对方的服务器给拒绝。

下面的示例表示一次发出 10 个异步请求，等它们全部结束后，才会重新再发 10 个请求。

```javascript
async.mapLimit(files, 10, async file => { // <- no callback!
    const text = await util.promisify(fs.readFile)(dir + file, 'utf8')
    const body = JSON.parse(text) // <- a parse error here will be caught automatically
    if (!(await checkValidity(body))) {
        throw new Error(`${file} has invalid contents`) // <- this error will also be caught
    }
    return body // <- return a value!
}, (err, contents) => {
    if (err) throw err
    console.log(contents)
})
```

## 2. 关键代码

```javascript
/**
 * 获取每一页的第一部分。PS：微博网页版每一页分三部分。
 */
async fetchPartFirst() {
    console.log(chalk.blueBright(`--开始下载第${this.pageIndex}页--`))
    const query = {
        profile_ftype: 1,
        is_all: 1,
        page: this.pageIndex,
    }
    let res = ''
    try {
        res = await request.get(FIRST_PART_API).query(query).set({ cookie: COOKIE })
    } catch (err) {
        console.error('获取每页第一部分片段失败', err)
        return
    }
    const $ = cheerio.load(res.text)
    const scripts = $('script')
    let rawHtml = ''
    for (let i = 0; i < scripts.length; i++) {
        const html = $(scripts[i]).html()
        if (html.includes('FM.view({"ns":"pl.content.homeFeed.index","domid":"Pl_Official_')) {
            rawHtml = html
        }
        if (html.includes('var $CONFIG = {};')) {
            global.eval(html)
            this.config = $CONFIG
        }
    }
    // 解析FM.view(...)
    const rawData = rawHtml.slice(8, -1)
    const data = JSON.parse(rawData)
    this.domid = data.domid
    const { imageUrls, videoUrls } = this.parseHtml(data.html)
    await async.mapLimit(imageUrls, THREAD_COUNT, async (item) => {
        return await this.saveImage(item)
    })
    await async.mapLimit(videoUrls, THREAD_COUNT, async (item) => {
        return await this.saveVideo(item)
    })
    await this.fetchPartOther()
}
```

```javascript
/**
 * 获取每一页的第二、三部分。主要由pagebar参数区分，0表示第二部分，1表示第三部分
 * @param {*} pagebar
 */
async fetchPartOther(pagebar = 0) {
    const query = {
        ajwvr: 6,
        domain: this.config.domain,
        is_search: 0,
        visible: 0,
        is_all: 1,
        is_tag: 0,
        profile_ftype: 1,
        page: this.pageIndex,
        pagebar: pagebar,
        pl_name: this.domid,
        id: `${this.config.domain}${this.config.oid}`,
        script_uri: SCRIPT_URI,
        feed_type: 0,
        pre_page: this.pageIndex,
        domain_op: this.config.domain,
        __rnd: Date.now(),
    }
    let res = ''
    try {
        res = await request.get(OTHER_PART_API).query(query).set({ cookie: COOKIE })
    } catch (err) {
        console.error('获取每页第二、三部分片段失败', err)
        return
    }
    const html = res.body.data
    if (this.pageCount < 0 && pagebar === 1) {
        this.initPageCount(html)
    }
    const { imageUrls, videoUrls } = this.parseHtml(html)
    await async.mapLimit(imageUrls, THREAD_COUNT, async (item) => {
        return await this.saveImage(item)
    })
    await async.mapLimit(videoUrls, THREAD_COUNT, async (item) => {
        return await this.saveVideo(item)
    })
    if (pagebar === 0) {
        // 获取该页第三部分片段
        await this.fetchPartOther(1)
    } else if (this.pageIndex < this.pageCount) {
        // 获取下一页内容
        this.pageIndex++
        await this.init()
    }
}
```

```javascript
/**
 * 获取总页数
 * @param {*} html
 */
initPageCount(html) {
    const $ = cheerio.load(html)
    const list = $('div[action-type=feed_list_page_morelist]').find('li')
    if (list) {
        this.pageCount = list.length
    }
}
```

```javascript
/**
 * 解析html片段，分离出图片路径
 * @param {*} html
 */
parseHtml(html) {
    const $ = cheerio.load(html)
    const list = $('div[action-type=feed_list_item]')
    const imageUrls = []
    const videoUrls = []
    list.map((index, item) => {
        const imgs = $(item).find('li[action-type=fl_pics] img')
        imgs.map((index, img) => {
            try {
                const path = $(img).attr('src')

                let url = path.replace('thumb150', 'mw690').replace('orj360', 'mw690')
                url = url.indexOf('http') > -1 ? url : 'http:' + url
                imageUrls.push(url)
            } catch (err) {
                console.error('提取图片路径失败', err)
            }
        })

        const videos = $(item).find('li[node-type=fl_h5_video]')
        videos.map((index, video) => {
            try {
                let path = $(video).prop('video-sources')
                path = path.replace('fluency=', '')
                path = decodeURIComponent(decodeURIComponent(path))
                videoUrls.push(path)
            } catch (err) {
                console.error('提取视频路径失败', err)
            }
        })
    })

    return { imageUrls, videoUrls }
}
```

```javascript
/**
 * 保存图片到本地
 * @param {*} url
 */
async saveImage(url) {
    const matchRes = url.match(/[^\/]+\.jpg/)
    if (!matchRes) {
        return
    }
    const name = matchRes[0]
    const userPath = path.resolve(__dirname, 'assets', this.config.title_value)
    if (!fs.existsSync(userPath)) {
        fs.mkdirSync(userPath)
    }
    const filePath = path.resolve(userPath, name)
    if (fs.existsSync(filePath)) {
        console.log(chalk.yellow(`图片${name}已经存在！`))
    } else {
        try {
            const res = await request.get(url)
            fs.writeFileSync(filePath, res.body)
            console.log(chalk.green(`图片${name}保存成功！`))
        } catch (err) {
            console.error(`图片${name}保存失败`, err)
        }
    }
}
```
```javascript
/**
 * 保存视频到本地
 * @param {*} url
 */
async saveVideo(url) {
    const matchRes = url.match(/[^\/]+\.mp4/)
    if (!matchRes) {
        return
    }
    const name = matchRes[0]
    const userPath = path.resolve(__dirname, 'assets', this.config.title_value)
    if (!fs.existsSync(userPath)) {
        fs.mkdirSync(userPath)
    }
    const filePath = path.resolve(userPath, name)
    if (fs.existsSync(filePath)) {
        console.log(chalk.yellow(`视频${name}已经存在！`))
    } else {
        try {
            const res = await request.get(url)
            fs.writeFileSync(filePath, res.body)
            console.log(chalk.green(`视频${name}保存成功！`))
        } catch (err) {
            console.error(`视频${name}保存失败`, err)
        }
    }
}
```

## 3. 源码地址

如果你对于爬虫不是很理解，看完上面关键代码后，发现还是无法爬取喜欢的小姐姐的图片和视频。你可以去博主的 github 查看源码，地址：[https://github.com/xikong1995/node-crawler](https://github.com/xikong1995/node-crawler)。

# 总结

以上就是今天要讲的内容，本文主要的精力花在了对爬虫分析上的讲解。写作过程中，发现了一个缺点。图文似乎不好很好的表达整个过程，如果真的想让小白朋友学会，那需要花的精力太多了。因为每一个细节其实要花大量的时间去调试，如果把它全部展现出来，那估计远不止几个小时。

其实吧，录视频反而更好，这样才能完整的带着大家一步步去分析，去调试。不过呢？录视频花的精力更多，恐怕只有真正靠这个吃饭的人才愿意去做。博主用爱发电，希望粗糙的讲解能给予大家一点点帮助。
